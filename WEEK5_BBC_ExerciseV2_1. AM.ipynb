{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "coursera": {
      "course_slug": "natural-language-processing-tensorflow",
      "graded_item_id": "3n7IN",
      "launcher_item_id": "I08yU"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abigailmwaura26/Mwaura_IT533/blob/main/WEEK5_BBC_ExerciseV2_1.%20AM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnwiOnGyW5JK"
      },
      "source": [
        "\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from os import getcwd\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYw-e9pDGdL_"
      },
      "source": [
        "This Python notebook is from *Exercise 2 BBC News Archive* in the Coursera class *Natural Language Processing in Tensorflow*.\n",
        "\n",
        "Adapted by Jung Hee Kim and Michael Glass.\n",
        "\n",
        "## Use word embeddings and a neural network to classify BBC news articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaVpWZetzfpv",
        "outputId": "53cd53e0-88c9-4cf5-e687-e0dd56c7e70b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Fetch the dataset of BBC news articles, with six different categories\n",
        "# You can read about this dataset here: https://www.kaggle.com/yufengdev/bbc-fulltext-and-category\n",
        "#\n",
        "!wget https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\n",
        "path_bbc = \"bbc-text.csv\""
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-08 21:05:08--  https://storage.googleapis.com/dataset-uploader/bbc/bbc-text.csv\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.69.207, 173.194.79.207, 108.177.96.207, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.69.207|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5057493 (4.8M) [text/csv]\n",
            "Saving to: ‘bbc-text.csv’\n",
            "\n",
            "bbc-text.csv        100%[===================>]   4.82M  6.47MB/s    in 0.7s    \n",
            "\n",
            "2025-02-08 21:05:10 (6.47 MB/s) - ‘bbc-text.csv’ saved [5057493/5057493]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYo6A4v5ZABQ"
      },
      "source": [
        "# Here are parameters that you can adjust for this lab.\n",
        "#\n",
        "# Parameters for producing sequences of number-tokens\n",
        "#\n",
        "vocab_size = 1000   # Number of most-common vocabulary words to recognize\n",
        "max_length = 120    # Standardized length (in word-tokens) of one article\n",
        "trunc_type = 'post' # Longer articles are truncated after max_length words\n",
        "padding_type = 'post' # Shorter articles are padded at the right end\n",
        "oov_tok = '<oov> '  # Uncommon words are replaced with this fake-word\n",
        "\n",
        "# For splitting between training and testing\n",
        "training_portion = .8\n",
        "\n",
        "# Size of embedding vectors for neural network processing\n",
        "embedding_dim = 22"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77fCI5gx7ZgR",
        "outputId": "da8a0517-d6c1-431b-e51c-16d341159d1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#  Function to prepare the text of an article.\n",
        "#  Right now it simply:\n",
        "#    -- makes lower-case\n",
        "#    -- removes extra white-space, mulitple spaces are condensed to single space\n",
        "#\n",
        "#  You can also update the function to return the article with stop words removed\n",
        "#\n",
        "def prepareText(s):\n",
        "  # Lowercase\n",
        "  t = s.lower()\n",
        "  # Split into list of words.  This will eliminate all the white space.\n",
        "  tlist = t.split()\n",
        "  #\n",
        "  # CODE FOR REMOVING STOPWORDS HERE.  Remove comment.\n",
        "  tlist = [w for w in tlist if w not in stopwords]\n",
        "  #\n",
        "  # Then rejoin into a single article string, with a single space between words\n",
        "  t = ' '.join(tlist)\n",
        "  return t\n",
        "\n",
        "#\n",
        "#  Stopwords are very common words, which often do not contribute any\n",
        "#  information to the task at hand. They are often deleted.\n",
        "#  This is a common list of stop words.\n",
        "stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "\n",
        "# Test the prepareText function\n",
        "#\n",
        "testArticle = \"I read it in  The  Times of London\"\n",
        "print(prepareText(testArticle))\n",
        "\n",
        "#   Originally:  I read it in  The  Times of London\n",
        "# Should print:  i read it in the times of london   (with no stopword removal)\n",
        "# Should print:  read times london  (with stopword removal)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "read times london\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU1qq3_SZBx_"
      },
      "source": [
        "# Variables for reading and preparing the text\n",
        "#\n",
        "# One article is stored as the text of a whole BBC article as a single string.\n",
        "#\n",
        "articles = []           # List of articles in original form.\n",
        "articlesPrepared = []   # Articles with texts prepared for tokenization\n",
        "#\n",
        "#  Each article has a label like \"business\", \"sport\", \"tech\" etc.\n",
        "labels = []\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eutB2xMiZD0e",
        "outputId": "7d11e841-58e6-44d3-cbaa-47eb6ac41cea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# The BBC data is in a CSV spreadsheet file, one article in each spreadsheet row.\n",
        "# The two spreadsheet column headings of interest are 'text' and 'category'\n",
        "#\n",
        "# Read the spreadsheet and make three lists, with one entry for each article.\n",
        "#\n",
        "with open(path_bbc, 'r') as csvfile:\n",
        "    csv_reader = csv.DictReader(csvfile)\n",
        "    for item in csv_reader:\n",
        "        articles.append(item['text'])  # Article text\n",
        "        articlesPrepared.append(prepareText(item['text'])) # Prepared version\n",
        "        labels.append(item['category'])  # Article category label\n",
        "\n",
        "# Print the label and the texts of the first few articles\n",
        "for i in range(3):\n",
        "  print(labels[i])\n",
        "  print(articles[i])\n",
        "  print(articlesPrepared[i])\n",
        "  print()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tech\n",
            "tv future in the hands of viewers with home theatre systems  plasma high-definition tvs  and digital video recorders moving into the living room  the way people watch tv will be radically different in five years  time.  that is according to an expert panel which gathered at the annual consumer electronics show in las vegas to discuss how these new technologies will impact one of our favourite pastimes. with the us leading the trend  programmes and other content will be delivered to viewers via home networks  through cable  satellite  telecoms companies  and broadband service providers to front rooms and portable devices.  one of the most talked-about technologies of ces has been digital and personal video recorders (dvr and pvr). these set-top boxes  like the us s tivo and the uk s sky+ system  allow people to record  store  play  pause and forward wind tv programmes when they want.  essentially  the technology allows for much more personalised tv. they are also being built-in to high-definition tv sets  which are big business in japan and the us  but slower to take off in europe because of the lack of high-definition programming. not only can people forward wind through adverts  they can also forget about abiding by network and channel schedules  putting together their own a-la-carte entertainment. but some us networks and cable and satellite companies are worried about what it means for them in terms of advertising revenues as well as  brand identity  and viewer loyalty to channels. although the us leads in this technology at the moment  it is also a concern that is being raised in europe  particularly with the growing uptake of services like sky+.  what happens here today  we will see in nine months to a years  time in the uk   adam hume  the bbc broadcast s futurologist told the bbc news website. for the likes of the bbc  there are no issues of lost advertising revenue yet. it is a more pressing issue at the moment for commercial uk broadcasters  but brand loyalty is important for everyone.  we will be talking more about content brands rather than network brands   said tim hanlon  from brand communications firm starcom mediavest.  the reality is that with broadband connections  anybody can be the producer of content.  he added:  the challenge now is that it is hard to promote a programme with so much choice.   what this means  said stacey jolna  senior vice president of tv guide tv group  is that the way people find the content they want to watch has to be simplified for tv viewers. it means that networks  in us terms  or channels could take a leaf out of google s book and be the search engine of the future  instead of the scheduler to help people find what they want to watch. this kind of channel model might work for the younger ipod generation which is used to taking control of their gadgets and what they play on them. but it might not suit everyone  the panel recognised. older generations are more comfortable with familiar schedules and channel brands because they know what they are getting. they perhaps do not want so much of the choice put into their hands  mr hanlon suggested.  on the other end  you have the kids just out of diapers who are pushing buttons already - everything is possible and available to them   said mr hanlon.  ultimately  the consumer will tell the market they want.   of the 50 000 new gadgets and technologies being showcased at ces  many of them are about enhancing the tv-watching experience. high-definition tv sets are everywhere and many new models of lcd (liquid crystal display) tvs have been launched with dvr capability built into them  instead of being external boxes. one such example launched at the show is humax s 26-inch lcd tv with an 80-hour tivo dvr and dvd recorder. one of the us s biggest satellite tv companies  directtv  has even launched its own branded dvr at the show with 100-hours of recording capability  instant replay  and a search function. the set can pause and rewind tv for up to 90 hours. and microsoft chief bill gates announced in his pre-show keynote speech a partnership with tivo  called tivotogo  which means people can play recorded programmes on windows pcs and mobile devices. all these reflect the increasing trend of freeing up multimedia so that people can watch what they want  when they want.\n",
            "tv future hands viewers home theatre systems plasma high-definition tvs digital video recorders moving living room way people watch tv will radically different five years time. according expert panel gathered annual consumer electronics show las vegas discuss new technologies will impact one favourite pastimes. us leading trend programmes content will delivered viewers via home networks cable satellite telecoms companies broadband service providers front rooms portable devices. one talked-about technologies ces digital personal video recorders (dvr pvr). set-top boxes like us s tivo uk s sky+ system allow people record store play pause forward wind tv programmes want. essentially technology allows much personalised tv. also built-in high-definition tv sets big business japan us slower take off europe lack high-definition programming. not can people forward wind adverts can also forget abiding network channel schedules putting together a-la-carte entertainment. us networks cable satellite companies worried means terms advertising revenues well brand identity viewer loyalty channels. although us leads technology moment also concern raised europe particularly growing uptake services like sky+. happens today will see nine months years time uk adam hume bbc broadcast s futurologist told bbc news website. likes bbc no issues lost advertising revenue yet. pressing issue moment commercial uk broadcasters brand loyalty important everyone. will talking content brands rather network brands said tim hanlon brand communications firm starcom mediavest. reality broadband connections anybody can producer content. added: challenge now hard promote programme much choice. means said stacey jolna senior vice president tv guide tv group way people find content want watch simplified tv viewers. means networks us terms channels take leaf google s book search engine future instead scheduler help people find want watch. kind channel model might work younger ipod generation used taking control gadgets play them. might not suit everyone panel recognised. older generations comfortable familiar schedules channel brands know getting. perhaps not want much choice put hands mr hanlon suggested. end kids just diapers pushing buttons already - everything possible available said mr hanlon. ultimately consumer will tell market want. 50 000 new gadgets technologies showcased ces many enhancing tv-watching experience. high-definition tv sets everywhere many new models lcd (liquid crystal display) tvs launched dvr capability built instead external boxes. one example launched show humax s 26-inch lcd tv 80-hour tivo dvr dvd recorder. one us s biggest satellite tv companies directtv even launched branded dvr show 100-hours recording capability instant replay search function. set can pause rewind tv 90 hours. microsoft chief bill gates announced pre-show keynote speech partnership tivo called tivotogo means people can play recorded programmes windows pcs mobile devices. reflect increasing trend freeing multimedia people can watch want want.\n",
            "\n",
            "business\n",
            "worldcom boss  left books alone  former worldcom boss bernie ebbers  who is accused of overseeing an $11bn (£5.8bn) fraud  never made accounting decisions  a witness has told jurors.  david myers made the comments under questioning by defence lawyers who have been arguing that mr ebbers was not responsible for worldcom s problems. the phone company collapsed in 2002 and prosecutors claim that losses were hidden to protect the firm s shares. mr myers has already pleaded guilty to fraud and is assisting prosecutors.  on monday  defence lawyer reid weingarten tried to distance his client from the allegations. during cross examination  he asked mr myers if he ever knew mr ebbers  make an accounting decision  .  not that i am aware of   mr myers replied.  did you ever know mr ebbers to make an accounting entry into worldcom books   mr weingarten pressed.  no   replied the witness. mr myers has admitted that he ordered false accounting entries at the request of former worldcom chief financial officer scott sullivan. defence lawyers have been trying to paint mr sullivan  who has admitted fraud and will testify later in the trial  as the mastermind behind worldcom s accounting house of cards.  mr ebbers  team  meanwhile  are looking to portray him as an affable boss  who by his own admission is more pe graduate than economist. whatever his abilities  mr ebbers transformed worldcom from a relative unknown into a $160bn telecoms giant and investor darling of the late 1990s. worldcom s problems mounted  however  as competition increased and the telecoms boom petered out. when the firm finally collapsed  shareholders lost about $180bn and 20 000 workers lost their jobs. mr ebbers  trial is expected to last two months and if found guilty the former ceo faces a substantial jail sentence. he has firmly declared his innocence.\n",
            "worldcom boss left books alone former worldcom boss bernie ebbers accused overseeing $11bn (£5.8bn) fraud never made accounting decisions witness told jurors. david myers made comments questioning defence lawyers arguing mr ebbers not responsible worldcom s problems. phone company collapsed 2002 prosecutors claim losses hidden protect firm s shares. mr myers already pleaded guilty fraud assisting prosecutors. monday defence lawyer reid weingarten tried distance client allegations. cross examination asked mr myers ever knew mr ebbers make accounting decision . not aware mr myers replied. ever know mr ebbers make accounting entry worldcom books mr weingarten pressed. no replied witness. mr myers admitted ordered false accounting entries request former worldcom chief financial officer scott sullivan. defence lawyers trying paint mr sullivan admitted fraud will testify later trial mastermind behind worldcom s accounting house cards. mr ebbers team meanwhile looking portray affable boss admission pe graduate economist. whatever abilities mr ebbers transformed worldcom relative unknown $160bn telecoms giant investor darling late 1990s. worldcom s problems mounted however competition increased telecoms boom petered out. firm finally collapsed shareholders lost $180bn 20 000 workers lost jobs. mr ebbers trial expected last two months found guilty former ceo faces substantial jail sentence. firmly declared innocence.\n",
            "\n",
            "sport\n",
            "tigers wary of farrell  gamble  leicester say they will not be rushed into making a bid for andy farrell should the great britain rugby league captain decide to switch codes.   we and anybody else involved in the process are still some way away from going to the next stage   tigers boss john wells told bbc radio leicester.  at the moment  there are still a lot of unknowns about andy farrell  not least his medical situation.  whoever does take him on is going to take a big  big gamble.  farrell  who has had persistent knee problems  had an operation on his knee five weeks ago and is expected to be out for another three months. leicester and saracens are believed to head the list of rugby union clubs interested in signing farrell if he decides to move to the 15-man game.  if he does move across to union  wells believes he would better off playing in the backs  at least initially.  i m sure he could make the step between league and union by being involved in the centre   said wells.  i think england would prefer him to progress to a position in the back row where they can make use of some of his rugby league skills within the forwards.  the jury is out on whether he can cross that divide.  at this club  the balance will have to be struck between the cost of that gamble and the option of bringing in a ready-made replacement.\n",
            "tigers wary farrell gamble leicester say will not rushed making bid andy farrell great britain rugby league captain decide switch codes. anybody else involved process still way away going next stage tigers boss john wells told bbc radio leicester. moment still lot unknowns andy farrell not least medical situation. whoever take going take big big gamble. farrell persistent knee problems operation knee five weeks ago expected another three months. leicester saracens believed head list rugby union clubs interested signing farrell decides move 15-man game. move across union wells believes better off playing backs least initially. m sure make step league union involved centre said wells. think england prefer progress position back row can make use rugby league skills within forwards. jury whether can cross divide. club balance will struck cost gamble option bringing ready-made replacement.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfdaWh06ZGe3"
      },
      "source": [
        "# Now split into training and validation sets\n",
        "# Use the training_portion variable.\n",
        "#\n",
        "train_size =  # YOUR CODE HERE\n",
        "\n",
        "train_articles = # YOUR CODE HERE\n",
        "train_labels = # YOUR CODE HERE\n",
        "\n",
        "validation_articles =  # YOUR CODE HERE\n",
        "validation_labels =  # YOUR CODE HERE\n",
        "\n",
        "print(train_size)\n",
        "print(len(train_articles))\n",
        "print(len(train_labels))\n",
        "print(len(validation_articles))\n",
        "print(len(validation_labels))\n",
        "\n",
        "# Expected output (if training_portion=0.8)\n",
        "# 1780\n",
        "# 1780\n",
        "# 1780\n",
        "# 445\n",
        "# 445"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULzA8xhwZI22"
      },
      "source": [
        "# The training articles: tokenize and turn into sequences of word-tokens\n",
        "#\n",
        "# Make a tokenizer object.\n",
        "#   It will utilize the vocab_size most common words\n",
        "#   The less common words will be repleaced with the oov_tok fake word\n",
        "#\n",
        "tokenizer =  # YOUR CODE HERE\n",
        "#\n",
        "#  Use your training articles to 'fit' the tokenizer. It will count all the words,\n",
        "#  assign numbers to the most common words.\n",
        "#\n",
        "tokenizer.   # YOUR CODE HERE\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Convert your training articles to sequences of number-tokens.\n",
        "# Then standardize the length of the sequences, truncating or padding as needed\n",
        "train_sequences =  # YOUR CODE HERE\n",
        "train_padded = # YOUR CODE HERE\n",
        "\n",
        "print(len(train_sequences[0]))\n",
        "\n",
        "print(len(train_padded[0]))\n",
        "\n",
        "print(len(train_sequences[1]))\n",
        "print(len(train_padded[1]))\n",
        "\n",
        "print(len(train_sequences[10]))\n",
        "print(len(train_padded[10]))\n",
        "\n",
        "# Expected Ouput (assuming training_portion = 0.8)\n",
        "# 449\n",
        "# 120\n",
        "# 200\n",
        "# 120\n",
        "# 192\n",
        "# 120"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8PeFWzPZLW_"
      },
      "source": [
        "# Now do the same for the validation articles: tokenize and turn into sequences\n",
        "#\n",
        "validation_sequences =  # YOUR CODE HERE\n",
        "validation_padded =  # YOUR CODE HERE\n",
        "\n",
        "print(len(validation_sequences))\n",
        "print(validation_padded.shape)\n",
        "\n",
        "# Expected output\n",
        "# 445\n",
        "# (445, 120)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7Xc-uWxXhML"
      },
      "source": [
        "#  Here you can decode and print the sequences back into article texts.\n",
        "#  Try it for a few sentences.\n",
        "#\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_sentence(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
        "\n",
        "print(train_articles[0])   # Try several different articles, not only 0\n",
        "print(decode_sentence(train_sequences[0]))\n",
        "\n",
        "#expected decoded ouput, assuming stopword removal\n",
        "# tv future hands viewers home theatre systems plasma high-definition tvs digital video\n",
        "# tv future <oov>  <oov>  home theatre systems <oov>  high <oov>  <oov>  digital video"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkWiQ_FKZNp2"
      },
      "source": [
        "#  Here we convert the category words to numbers.\n",
        "#  \"sport\" will become 1, \"business\" will become 2, etc.\n",
        "#\n",
        "#  We can use the tokenizer as a kind of programming trick.\n",
        "#  We will 'fit' the tokenizer on the labels, each label is effectively a one-word sentence.\n",
        "#  So each category word will be assigned a number-token, the label will become a single number.\n",
        "#\n",
        "# First make a Tokenizer object and fit it.\n",
        "label_tokenizer = Tokenizer()\n",
        "label_tokenizer.fit_on_texts(labels)\n",
        "\n",
        "print(label_tokenizer.word_index)\n",
        "# Should print:\n",
        "#   {'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n",
        "\n",
        "# Convert the training and validation labels into number-tokens.\n",
        "# Then use np.array() to make 1-dimensional np arrays\n",
        "training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n",
        "validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))\n",
        "\n",
        "print(training_label_seq[0])\n",
        "print(training_label_seq[1])\n",
        "print(training_label_seq[2])\n",
        "print(training_label_seq.shape)\n",
        "\n",
        "print(validation_label_seq[0])\n",
        "print(validation_label_seq[1])\n",
        "print(validation_label_seq[2])\n",
        "print(validation_label_seq.shape)\n",
        "\n",
        "# Expected output\n",
        "# [4]\n",
        "# [2]\n",
        "# [1]\n",
        "# (1780, 1)\n",
        "# [5]\n",
        "# [4]\n",
        "# [3]\n",
        "# (445, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZ5um4MWZP-W"
      },
      "source": [
        "# Neural network!  Four layers.\n",
        "#\n",
        "#  First is an Embedding layer, which converts each number-token into a vector\n",
        "#  The output of the embedding layer is one vector for each word, so if each\n",
        "#  input sequence has 50 words (50 numbers-tokens) and the embedding dimension is 15,\n",
        "#  the output is a 2D tensor 50 x 15.\n",
        "#\n",
        "#  The next layer will either:\n",
        "#    a) Flatten the 2D tensor to 1-dimension, e.g. 750 numbers\n",
        "#    b) Average up the each dimension using GlobalAveragePooling1D.\n",
        "#       This will produce single vector representing an\n",
        "#       embedding of the average word, e.g. 15 numbers\n",
        "#\n",
        "#  Then comes a hidden layer, densely connected.\n",
        "#\n",
        "#  Finally a softmax output layer, one neuron for each label/category number.\n",
        "#  Output layer is six instead of five, as we are not using category number 0.\n",
        "#\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    #tf.keras.layers.GlobalAveragePooling1D(),    # YOUR CODE HERE: uncomment to\n",
        "    #tf.keras.layers.Flatten(),                   #  pick Flatten() or GlobalAveragePooling1D()\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(6, activation='softmax')\n",
        "])\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Expected summary output with:\n",
        "#    120-word sequences and 16-number embedding vectors\n",
        "#    using GlobalAveragePooling for the 2nd layer\n",
        "#\n",
        "# Layer (type)                 Output Shape              Param #\n",
        "# =================================================================\n",
        "# embedding (Embedding)        (None, 120, 16)           16000\n",
        "# _________________________________________________________________\n",
        "# global_average_pooling1d (Gl (None, 16)                0\n",
        "# _________________________________________________________________\n",
        "# dense (Dense)                (None, 24)                408\n",
        "# _________________________________________________________________\n",
        "# dense_1 (Dense)              (None, 6)                 150\n",
        "# =================================================================\n",
        "# Total params: 16,558\n",
        "# Trainable params: 16,558\n",
        "# Non-trainable params: 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsfdxySKZSXu",
        "scrolled": true
      },
      "source": [
        "## Time to train and validate the neural network classifier, using model.fit()\n",
        "#\n",
        "# The validation data is tested after each training epoch. (We don't wait until training is done in order to test.)\n",
        "#\n",
        "num_epochs = 25\n",
        "history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ0BX2apXS9u"
      },
      "source": [
        "# These two graphs will show how the training and validation improved for each epoch.\n",
        "# It may show that the training data accuracy improves even after the validation data\n",
        "#  accuracy stopped improving.\n",
        "#\n",
        "#  This shows that the network is over-training a little bit.\n",
        "#\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhnFA_TDXrih"
      },
      "source": [
        "# For fun, if you are interested, you can extract the embedding vectors.\n",
        "# Here is how to extract them from the trained neural network.\n",
        "#\n",
        "e = model.layers[0]\n",
        "weights = e.get_weights()[0]\n",
        "print(weights.shape) # shape: (vocab_size, embedding_dim)\n",
        "\n",
        "# Expected output, using a maximum vocabulary of 1000 words and 16 element vectors\n",
        "# (1000, 16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we will compare the embeddings of several words of your choice**, using cosine similarity.\n",
        "\n",
        "Since this network was trained for a classification task using only 5 categories, generally the words which are indicative of a particular category should show vector similarity."
      ],
      "metadata": {
        "id": "8MBy-KhxBZ2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FIRST: Put your words here and test them\n",
        "\n",
        "words = ['word1', 'word2', 'word3', 'word4', 'word5', 'word6'] # put your words here\n",
        "words = ['london', 'people', 'digital', 'lost', 'rugby', 'player'] # put your words here\n",
        "for w in words:\n",
        "  if not w in word_index:\n",
        "    print(w, \"not recognized\")\n",
        "  elif word_index[w] >= vocab_size:\n",
        "    print(w, word_index[w], \"> vocab_size\")\n",
        "  else:\n",
        "    print(w, word_index[w], 'OK')\n"
      ],
      "metadata": {
        "id": "-1fa0F4vZkc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SECOND: Compute all comparisons\n",
        "\n",
        "# Compare two vectors with cosine: (A dot B) / (|A| * |B|)\n",
        "#\n",
        "def comp(A, B):\n",
        "  return np.dot(A,B) / (np.linalg.norm(A) * np.linalg.norm(B))\n",
        "\n",
        "# Print all comparisons\n",
        "for i in range(len(words)-1):\n",
        "  w1 = words[i]\n",
        "  v1 = weights[word_index[w1]]\n",
        "  for j in range(i+1,len(words)):\n",
        "    w2 = words[j]\n",
        "    v2 = weights[word_index[w2]]\n",
        "    print(\"cosine\", w1, w2, '=', comp(v1, v2))"
      ],
      "metadata": {
        "id": "Dkp_c4eNCx0G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}